global:
  cloudProvider: ""
  addPrometheusAnnotations: true

argocd-apps:
  enabled: true

job:
  image: docker.io/entigolabs/entigo-infralib-base
  tag: "v1.13.1"

argocd:
  crds:
    install: true
    keep: true
  global:
    domain: argocd.${domain}
    networkPolicy:
      create: true
  configs:
    params:
      server.insecure: false
    cm:
      accounts.infralib: apiKey
      application.resourceTrackingMethod: annotation
      resource.exclusions: |
            - apiGroups:
              - "*"
              kinds:
              - ProviderConfigUsage
      resource.customizations: |
        "*.upbound.io/*":
          health.lua: |
            health_status = {
            status = "Progressing",
            message = "Provisioning ..."
            }

            local function contains (table, val)
            for i, v in ipairs(table) do
                if v == val then
                return true
                end
            end
            return false
            end

            local has_no_status = {
            "ClusterProviderConfig",
            "ProviderConfig",
            "ProviderConfigUsage"
            }

            if obj.status == nil or next(obj.status) == nil and contains(has_no_status, obj.kind) then
            health_status.status = "Healthy"
            health_status.message = "Resource is up-to-date."
            return health_status
            end

            if obj.status == nil or next(obj.status) == nil or obj.status.conditions == nil then
            if (obj.kind == "ProviderConfig" or obj.kind == "ClusterProviderConfig") and obj.status.users ~= nil then
                health_status.status = "Healthy"
                health_status.message = "Resource is in use."
                return health_status
            end
            return health_status
            end

            for i, condition in ipairs(obj.status.conditions) do
            if condition.type == "LastAsyncOperation" then
                if condition.status == "False" then
                health_status.status = "Degraded"
                health_status.message = condition.message
                return health_status
                end
            end

            if condition.type == "Synced" then
                if condition.status == "False" then
                health_status.status = "Degraded"
                health_status.message = condition.message
                return health_status
                end
            end

            if condition.type == "Ready" then
                if condition.status == "True" then
                health_status.status = "Healthy"
                health_status.message = "Resource is up-to-date."
                return health_status
                end
            end
            end

            return health_status

        "*.crossplane.io/*":
          health.lua: |
            health_status = {
            status = "Progressing",
            message = "Provisioning ..."
            }

            local function contains (table, val)
            for i, v in ipairs(table) do
                if v == val then
                return true
                end
            end
            return false
            end

            local has_no_status = {
            "Composition",
            "CompositionRevision",
            "DeploymentRuntimeConfig",
            "ClusterProviderConfig",
            "ProviderConfig",
            "ProviderConfigUsage"
            }
            if obj.status == nil or next(obj.status) == nil and contains(has_no_status, obj.kind) then
                health_status.status = "Healthy"
                health_status.message = "Resource is up-to-date."
            return health_status
            end

            if obj.status == nil or next(obj.status) == nil or obj.status.conditions == nil then
            if (obj.kind == "ProviderConfig" or obj.kind == "ClusterProviderConfig") and obj.status.users ~= nil then
                health_status.status = "Healthy"
                health_status.message = "Resource is in use."
                return health_status
            end
            return health_status
            end

            for i, condition in ipairs(obj.status.conditions) do
            if condition.type == "LastAsyncOperation" then
                if condition.status == "False" then
                health_status.status = "Degraded"
                health_status.message = condition.message
                return health_status
                end
            end

            if condition.type == "Synced" then
                if condition.status == "False" then
                health_status.status = "Degraded"
                health_status.message = condition.message
                return health_status
                end
            end

            if contains({"Ready", "Healthy", "Offered", "Established", "ValidPipeline", "RevisionHealthy"}, condition.type) then
                if condition.status == "True" then
                health_status.status = "Healthy"
                health_status.message = "Resource is up-to-date."
                return health_status
                end
            end
            end

            return health_status
      server.rbac.log.enforce.enable: "true"
      admin.enabled: "true"
      exec.enabled: "true"
    rbac:
      policy.csv: |
        p, infralib, applications, get, */*, allow
        p, infralib, applications, sync, */*, allow
        p, infralib, projects, get, *, allow
  server:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            preference:
              matchExpressions:
                - key: tools
                  operator: In
                  values:
                    - "true"
    tolerations:
      - key: "tools"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
    resources:
      limits:
        cpu: 1000m
        memory: 512Mi
        ephemeral-storage: 1Gi
      requests:
        cpu: 40m
        memory: 96Mi
        ephemeral-storage: 100Mi
    autoscaling:
      enabled: true
      minReplicas: 1
      maxReplicas: 3
      targetMemoryUtilizationPercentage: 80
      targetCPUUtilizationPercentage: 90
  dex:
    deploymentAnnotations:
      kube-score/ignore: pod-probes
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            preference:
              matchExpressions:
                - key: tools
                  operator: In
                  values:
                    - "true"
    tolerations:
      - key: "tools"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
    resources:
      limits:
        cpu: 1000m
        memory: 512Mi
        ephemeral-storage: 20Gi
      requests:
        cpu: 10m
        memory: 48Mi
        ephemeral-storage: 2Gi

  redis:
    enabled: true
    deploymentAnnotations:
      kube-score/ignore: pod-probes
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            preference:
              matchExpressions:
                - key: tools
                  operator: In
                  values:
                    - "true"
    tolerations:
      - key: "tools"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
    resources:
      limits:
        cpu: 1000m
        memory: 128Mi
        ephemeral-storage: 1Gi
      requests:
        cpu: 30m
        memory: 32Mi
        ephemeral-storage: 100Mi
  redisSecretInit:
    jobAnnotations:
      kube-score/ignore: pod-networkpolicy
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: HookSucceeded
    podAnnotations:
      sidecar.istio.io/inject: "false"
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            preference:
              matchExpressions:
                - key: tools
                  operator: In
                  values:
                    - "true"
    tolerations:
      - key: "tools"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
    resources:
      limits:
        cpu: 1000m
        memory: 128Mi
        ephemeral-storage: 1Gi
      requests:
        cpu: 10m
        memory: 64Mi
        ephemeral-storage: 1Mi

  redis-ha:
    enabled: false
    redis:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 90
              preference:
                matchExpressions:
                  - key: tools
                    operator: In
                    values:
                      - "true"
      tolerations:
        - key: "tools"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      resources:
        limits:
          cpu: 1000m
          memory: 128Mi
          ephemeral-storage: 1Gi
        requests:
          cpu: 70m
          memory: 128Mi
          ephemeral-storage: 100Mi
    sentinel:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 90
              preference:
                matchExpressions:
                  - key: tools
                    operator: In
                    values:
                      - "true"
      tolerations:
        - key: "tools"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      resources:
        limits:
          cpu: 1000m
          memory: 128Mi
          ephemeral-storage: 1Gi
        requests:
          cpu: 70m
          memory: 128Mi
          ephemeral-storage: 100Mi
    haproxy:
      affinity: |
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            preference:
              matchExpressions:
              - key: tools
                operator: In
                values:
                - "true"
      tolerations:
        - key: "tools"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      resources:
        limits:
          cpu: 1000m
          memory: 128Mi
          ephemeral-storage: 1Gi
        requests:
          cpu: 70m
          memory: 128Mi
          ephemeral-storage: 100Mi

  repoServer:
    autoscaling:
      enabled: true
      minReplicas: 1
      maxReplicas: 3
      targetMemoryUtilizationPercentage: 80
      targetCPUUtilizationPercentage: 95

    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            preference:
              matchExpressions:
                - key: tools
                  operator: In
                  values:
                    - "true"
    tolerations:
      - key: "tools"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
    resources:
      limits:
        cpu: 1000m
        memory: 512Mi
        ephemeral-storage: 1Gi
      requests:
        cpu: 70m
        memory: 96Mi
        ephemeral-storage: 100Mi

  applicationSet:
    replicaCount: 2
    deploymentAnnotations:
      kube-score/ignore: pod-networkpolicy,pod-probes
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            preference:
              matchExpressions:
                - key: tools
                  operator: In
                  values:
                    - "true"
    tolerations:
      - key: "tools"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
    resources:
      limits:
        cpu: 1000m
        memory: 512Mi
        ephemeral-storage: 1Gi
      requests:
        cpu: 10m
        memory: 32Mi
        ephemeral-storage: 100Mi

  notifications:
    deploymentAnnotations:
      kube-score/ignore: pod-networkpolicy
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            preference:
              matchExpressions:
                - key: tools
                  operator: In
                  values:
                    - "true"
    tolerations:
      - key: "tools"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
    resources:
      limits:
        cpu: 1000m
        memory: 128Mi
        ephemeral-storage: 1Gi
      requests:
        cpu: 10m
        memory: 32Mi
        ephemeral-storage: 100Mi

  controller:
    env: 
    - name: ARGOCD_K8S_CLIENT_QPS
      value: "150"
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            preference:
              matchExpressions:
                - key: tools
                  operator: In
                  values:
                    - "true"
    tolerations:
      - key: "tools"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
    resources:
      limits:
        cpu: 3000m
        memory: 4096Mi
        ephemeral-storage: 1Gi
      requests:
        cpu: 70m
        memory: 768Mi
        ephemeral-storage: 100Mi
